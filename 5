rm(list = ls())


## load the MASS library
library(MASS) 

#################################################
## define some functions
#################################################
#################################################
### sample from a multivariate normal distribution
rmvnorm<-
function(n,mu,Sigma) {
  p<-length(mu)
  res<-matrix(0,nrow=n,ncol=p)
  if( n>0 & p>0 ) {
  E<-matrix(rnorm(n*p),n,p)
  res<-t(  t(E%*%chol(Sigma)) +c(mu))
                   }
  res
                       }
###


### sample from an inverse-wishart distribution
rinvwish<-function(n,nu0,iS0) 
{
  sL0 <- chol(iS0) 
  S<-array( dim=c( dim(L0),n ) )
  for(i in 1:n) 
  {
     Z <- matrix(rnorm(nu0 * dim(L0)[1]), nu0, dim(iS0)[1]) %*% sL0  
     S[,,i]<- solve(t(Z)%*%Z)
  }     
  S[,,1:n]
}
###

### log of a mmultivariate normal density
ldmvnorm<-function(y,mu,Sig){  # log mvn density
       c(  -(length(mu)/2)*log(2*pi) -.5*log(det(Sig)) -.5*
            t(y-mu)%*%solve(Sig)%*%(y-mu)   )  
                             }
####

### sample from the Wishart distribution
rwish<-function(n,nu0,S0)
{
  sS0 <- chol(S0)
  S<-array( dim=c( dim(S0),n ) )
  for(i in 1:n)
  {
     Z <- matrix(rnorm(nu0 * dim(S0)[1]), nu0, dim(S0)[1]) %*% sS0
     S[,,i]<- t(Z)%*%Z
  }
  S[,,1:n]
}
###




#####
Y <- read.table("Yreading.txt")

mu0 <- c(50,50)
L0 <- matrix( c(625,312.5,312.5,625),nrow=2,ncol=2)

nu0<-4
S0<-matrix( c(625,312.5,312.5,625),nrow=2,ncol=2)

n<-dim(Y)[1] ; ybar<-apply(Y,2,mean)
Sigma<-cov(Y) ; THETA<-SIGMA<-NULL
YS<-NULL
set.seed(1)

for(s in 1:5000) 
{
 
  ###update theta
  Ln<-solve( solve(L0) + n*solve(Sigma) )
  mun<-Ln%*%( solve(L0)%*%mu0 + n*solve(Sigma)%*%ybar )
  theta<-rmvnorm(1,mun,Ln)  
  ### 
   
  ###update Sigma
  Sn<- S0 + ( t(Y)-c(theta) )%*%t( t(Y)-c(theta) ) 
  Sigma<-rinvwish(1,nu0+n,solve(Sn))
  
  #Sigma<-solve( rwish(1, nu0+n, solve(Sn)) )
  ###

  ### samples from the posterior predictive
  YS<-rbind(YS,rmvnorm(1,theta,Sigma)) 
  ###

  ### save results 
  THETA<-rbind(THETA,theta) ; SIGMA<-rbind(SIGMA,c(Sigma))
  ###
  cat(s,round(theta,2),round(c(Sigma),2),"\n")
}

quantile(  SIGMA[,2]/sqrt(SIGMA[,1]*SIGMA[,4]), prob=c(.025,.5,.975) )
quantile(   THETA[,2]-THETA[,1], prob=c(.025,.5,.975) )
mean( THETA[,2]-THETA[,1])
mean( THETA[,2]>THETA[,1]) 
mean(YS[,2]>YS[,1])


## plots of the results
par(mfrow=c(1,2))

plot(THETA[,1], THETA[,2], xlab=expression(theta[1]), ylab=expression(theta[2]), pch=16, col="blue")
abline(0,1, lwd=2)

plot(YS[,1], YS[,2], xlab=expression(italic(y[1])),ylab=expression(italic(y[2])), xlim=c(0,100), ylim=c(0,100), pch=16, col="blue")
abline(0,1, lwd=2)


#################################################
#################################################
# Missing Data Example
##### get data
Y <- read.table("Ypimamiss.txt")
n <- nrow(Y)
############################


### prior parameters
p<-dim(Y)[2]
mu0<-c(120,64,26,26)
sd0<-(mu0/2)
L0<-matrix(.1,p,p) ; diag(L0)<-1 ; L0<-L0*outer(sd0,sd0)
nu0<-p+2 ; S0 <- L0
###

### starting values
Sigma<-S0
Y.full<-Y
O <- 1*(!is.na(Y))
for(j in 1:p)
{
  Y.full[is.na(Y.full[,j]),j]<-mean(Y.full[,j],na.rm=TRUE)
}
###


### Gibbs sampler
THETA<-SIGMA<-Y.MISS<-NULL
set.seed(1)



for(s in 1:1000)
{

  ### update theta
  ybar<-apply(Y.full,2,mean)
  Ln<-solve( solve(L0) + n*solve(Sigma) )
  mun<-Ln%*%( solve(L0)%*%mu0 + n*solve(Sigma)%*%ybar )
  theta<-rmvnorm(1,mun,Ln)
  ###
  
  ### update Sigma
  Sn<- S0 + ( t(Y.full)-c(theta) )%*%t( t(Y.full)-c(theta) )
#  Sigma<-rinvwish(1,nu0+n,solve(Sn))
  Sigma<-solve( rwish(1, nu0+n, solve(Sn)) )
  ###
  
  ### update missing data
  for(i in 1:n)
  { 
    b <- ( O[i,]==0 )
    a <- ( O[i,]==1 )
    iSa<- solve(Sigma[a,a])
    beta.j <- Sigma[b,a]%*%iSa
    s2.j   <- Sigma[b,b] - Sigma[b,a]%*%iSa%*%Sigma[a,b]
    theta.j<- theta[b] + beta.j%*%(t(Y.full[i,a])-theta[a])
    Y.full[i,b] <- rmvnorm(1, theta.j, s2.j )
		 }
  
  ### save results
  THETA<-rbind(THETA,theta) ; SIGMA<-rbind(SIGMA,c(Sigma))
  Y.MISS<-rbind(Y.MISS, Y.full[O==0] )
  ###

  cat(s,theta,"\n")

}

#############
## examine posterior summeries

## mean of posterior mean
apply(THETA,2,mean)


## calculate the corrletion matrix
COR <- array( dim=c(p,p,1000) )
for(s in 1:1000)
{
  Sig<-matrix( SIGMA[s,] ,nrow=p,ncol=p)
  COR[,,s] <- Sig/sqrt( outer( diag(Sig),diag(Sig) ) )
}

## mean of correlation matrix
apply(COR,c(1,2),mean)

## 95% ci of correlation matrix
apply(COR,c(1,2),quantile, c(0.025, 0.975))


#################
## Examine predictions vs observed values
#####
Y.true<- read.table("Ypimafull.txt")

V <- matrix(1:p,nrow=n,ncol=p,byrow=TRUE)
v.miss<-V[O==0]

y.pred <- apply(Y.MISS, 2, mean)
y.true <- Y.true[O==0]

par(mfrow=c(2,2))
for(j in 1:p){ 
		plot(y.true[v.miss==j], y.pred[v.miss==j], xlab=paste("true", colnames(Y.true)[j]), ylab=paste("predictied", colnames(Y.true)[j]),pch=16 )
		abline(0,1)
		cat(j, mean( (y.true[v.miss==j]- y.pred[v.miss==j])^2),"\n") 
		}
#####
