## clear all of the objects
rm(list = ls())


## load the MASS library
library(MASS) 

#################################################
## define some functions
#################################################
#################################################
### sample from a multivariate normal distribution
rmvnorm<-
function(n,mu,Sigma) {
	p<-length(mu)
	res<-matrix(0,nrow=n,ncol=p)
	if( n>0 & p>0 ) {
		E<-matrix(rnorm(n*p),n,p)
		res<-t(  t(E%*%chol(Sigma)) +c(mu))
	}
	res
}
###


### sample from an inverse-wishart distribution
rinvwish<-function(n,nu0,iS0) 
{
	sL0 <- chol(iS0) 
	S<-array( dim=c( dim(L0),n ) )
	for(i in 1:n) 
	{
		Z <- matrix(rnorm(nu0 * dim(L0)[1]), nu0, dim(iS0)[1]) %*% sL0  
		S[,,i]<- solve(t(Z)%*%Z)
	}     
	S[,,1:n]
}


## density of a multivariate normal 
dmvnorm<- function(y, mu, Sigma){
	p <- dim(Sigma)[1]
	out <- (2*pi)^(-p/2)*det(Sigma)^(-1/2)*exp( (-1/2)*t(y-mu)%*%solve(Sigma)%*%(y-mu))
	return(out)
}


##########################################################
## Data
x1<-c(0,0,0,0,0,0,1,1,1,1,1,1) ## 1 = aerobic program
x2<-c(23,22,22,25,27,20,31,23,27,28,22,24) ## age
y<-c(-0.87,-10.74,-3.27,-1.97,7.50,-7.25,17.05,4.96,10.40,11.05,0.26,2.51)

## EDA
par(mfrow=c(1,2))
plot(y ~ factor(x1), col=c("blue", "red"))

plot(x2,y, xlab="age")
points(x2[x1==0], y[x1==0], col="blue")
points(x2[x1==1], y[x1==1], col="red")

###########################################################
##### LS estimation 
n <- length(y)
X <- cbind(rep(1,n),x1,x2,x1*x2)
p <- dim(X)[2]

beta.ols<- solve(t(X)%*%X)%*%t(X)%*%y
sigma.sq.ols <- sum((y - X%*%beta.ols)^2)/(n-p) 

SE.beta.ols <- sqrt(diag(sigma.sq.ols*solve(t(X)%*%X)))

## test statistic for H0: Beta=0
beta.ols/SE.beta.ols

summary(lm(y ~ x1 + x2 + x1:x2))

##########################################################
## Set up Gibbs sampler based on unit information priors
s20 <- sum((y - X%*%beta.ols)^2)/(n-p) 
nu0 <- 1

beta0 <- beta.ols
Sigma0 <- solve(t(X)%*%X)*(n*s20)

## Storage matrices
BETA<-SIGMA2<-NULL

S <- 5000


## starting value for sigma.sq
sigma.sq <- s20


## Run the sampler
for(s in 1:S){

## update beta
V <- solve( solve(Sigma0) + t(X)%*%X/sigma.sq)
M <- V%*%(solve(Sigma0)%*%beta0 + t(X)%*%y/sigma.sq)
beta <- t(rmvnorm(1, M, V))
		
## update sigma.sq
SSR.b <- t(y - X%*%beta)%*%(y - X%*%beta)	
sigma.sq <- 1/rgamma(1, (nu0+n)/2, (nu0*s20 + SSR.b)/2)	
		  
		  
## store
BETA <- rbind(BETA, t(beta))
SIGMA2 <- rbind(SIGMA2, sigma.sq)	
	
# print	
cat(s, t(beta), sigma.sq, "\n")	
}

apply(BETA, 2, quantile, c(0.025, 0.5, 0.975))
apply(SIGMA2, 2, quantile, c(0.025, 0.5, 0.975))




###################################################################################################
## Set up Monte Carlo sampler based on the g prior for beta and unit information prior for sigma.sq

## priors
g <- length(y)
nu0 <- 1
s20 <- sum((y - X%*%beta.ols)^2)/(n-p)


S <- 5000
## 
Hg <- (g/(g+1))*X%*%solve(t(X)%*%X)%*%t(X)

SSRg <- t(y)%*%(diag(1, nrow=n) - Hg)%*%y

## sample sigma.sq
sigma.sq <- 1/rgamma(S, (nu0 + n)/2 , (nu0*s20 + SSRg)/2)

## sample beta
M <- (g/(g+1))*beta.ols


beta <- NULL
for(s in 1:S){
beta.s <- rmvnorm(1, M, (g/(g+1))*sigma.sq[s]*solve(t(X)%*%X))
beta <- rbind(beta, beta.s)	
}

beta.post.g <- beta
sigma.sq.post.g <- sigma.sq

## examine results
apply(beta.post.g, 2, quantile, c(0.025, 0.5, 0.975))
quantile(sigma.sq.post.g, c(0.025, 0.5, 0.975))


## further examination
age <- 20:30

Out <- matrix(0, S, length(20:30))

for(i in 1:length(20:30)){
Out[,i] <- 	beta.post.g[,2] + beta.post.g[,4]*age[i]
}


post.qu <- apply(Out, 2, quantile, c(0.025, 0.5, 0.975))

plot(age, post.qu[2,], type="n", ylim=c(-10, 20), xlab="age", ylab=expression(paste( beta[2] + beta[4],"age",sep="") ) )
segments(age, post.qu[1,], age, post.qu[3,], lwd=2, col="blue")
points(age, post.qu[2,], col="brown")
abline(h=0, col="red", lwd=2)



###############################################################################################
## calculate the marginal of y for this model analytically, Monte Carlo approach, and Newton and Raftery approach based on posteriors

#####################################
## analytical solution pg 165 of book
Hg <- (g/(g+1))*X%*%solve(t(X)%*%X)%*%t(X)
SSRg <- t(y)%*%(diag(1, nrow=n) - Hg)%*%y

marg.y.a <- pi^(-n/2)* ( gamma((nu0+n)/2)/gamma((nu0)/2))*((1+g)^(-p/2))*
          ( ((nu0*s20)^(nu0/2))/((nu0*s20 + SSRg)^((nu0+n)/2)))

marg.y.a
log(marg.y.a)


##### a function to compute the marginal probability
lpy.X <- function(y, X, g, nu0, s20)
{
	n <- dim(X)[1]; p <- dim(X)[2]
	if(p==0){Hg <- 0; s20 <- mean(y^2)}
	if(p>0) { Hg <- (g/(g+1))*X%*%solve(t(X)%*%X)%*%t(X)}
	SSRg <- t(y)%*%(diag(1, nrow=n) - Hg)%*%y 
	out <- -0.5*(n*log(pi)+p*log(1+g)+(nu0+n)*log(nu0*s20+SSRg) - nu0*log(nu0*s20)) + lgamma ((nu0+n)/2) - lgamma (nu0/2)
	return(out)
} 

lpy.X(y, X, g, nu0, s20)

#######################################################################
## marginal of y from sampling from monte carlo sampling from the g prior 
g <- 12
nu0 <- 1
s20 <- sum((y - X%*%beta.ols)^2)/(n-p) 
Beta0 <- matrix(rep(0, dim(X)[[2]]),ncol=1)

## monte carlo sampling
S <- 5000

## sample sigma.sq from prior
sigma.sq <- 1/rgamma(S, nu0/2, nu0*s20/2) 

## sample beta from prior
beta <- matrix(0, S, p)
for(i in 1:S){
beta[i,] <- rmvnorm(1, Beta0, g*sigma.sq[i]*solve(t(X)%*%X))
}				

## calculate mean marginal of y
marg.y.mc <- rep(0, S)
for(i in 1:S){
marg.y.mc[i] <- dmvnorm(y, X%*%beta[i,], sigma.sq[i]*diag(1, nrow=n))
}			

marg.y.mc <- mean(marg.y.mc)
log(marg.y.mc)

#########################################################################
## marginal of y from posterior samples and hamonic mean based on g-prior

post.lik <- rep(0, S)
for(i in 1:S){
	post.lik[i] <- dmvnorm(y, X%*%beta.post.g[i,], sigma.sq.post.g[i]*diag(1, nrow=n))
}



marg.y.NR <- 1/mean(1/post.lik)
log(marg.y.NR)


###################################################################
###################################################################
####################################################################
## Full Bayesian Model Selection Based on the g-prior 
## Combining code on pg 168 with the code above!!
## Using Diabetes data!

## clear all of the objects
rm(list = ls())

## load the MASS library
library(MASS) 

#################################################
## define some functions
#################################################
#################################################
### sample from a multivariate normal distribution
rmvnorm<-
function(n,mu,Sigma) {
	p<-length(mu)
	res<-matrix(0,nrow=n,ncol=p)
	if( n>0 & p>0 ) {
		E<-matrix(rnorm(n*p),n,p)
		res<-t(  t(E%*%chol(Sigma)) +c(mu))
	}
	res
}
###


### sample from an inverse-wishart distribution
rinvwish<-function(n,nu0,iS0) 
{
	sL0 <- chol(iS0) 
	S<-array( dim=c( dim(L0),n ) )
	for(i in 1:n) 
	{
		Z <- matrix(rnorm(nu0 * dim(L0)[1]), nu0, dim(iS0)[1]) %*% sL0  
		S[,,i]<- solve(t(Z)%*%Z)
	}     
	S[,,1:n]
}



##### a function to compute the marginal probability
lpy.X <- function(y, X, g, nu0, s20)
{
	n <- dim(X)[1]; p <- dim(X)[2]
	if(p==0){Hg <- 0; s20 <- mean(y^2)}
	if(p>0) { Hg <- (g/(g+1))*X%*%solve(t(X)%*%X)%*%t(X)}
	SSRg <- t(y)%*%(diag(1, nrow=n) - Hg)%*%y 
	out <- -0.5*(n*log(pi)+p*log(1+g)+(nu0+n)*log(nu0*s20+SSRg) - nu0*log(nu0*s20)) + lgamma ((nu0+n)/2) - lgamma (nu0/2)
	return(out)
} 



## Load Data
diabTrain <- read.table("diabTrain.dat", header=T)
diabTest <- read.table("diabTest.dat", header=T)

y <- as.matrix(diabTrain$y)
X <- as.matrix(diabTrain[,-1])
n <- dim(X)[1]
p <- dim(X)[2]

y.test <- as.matrix(diabTest$y.te)
X.test <- as.matrix(diabTest[,-1])

Beta.ols <- solve(t(X)%*%X)%*%t(X)%*%y
Beta.ols


## check of ols mse on pg 161
y.hat.test <- X.test%*%Beta.ols
mean( (y.test-y.hat.test)^2)

## just predict each case to be zero
mean( (y.test-0)^2)


## priors
g <- length(y)
nu0 <- 1
s20 <- sum((y - X%*%Beta.ols)^2)/(n-p)

## starting values
z<-rep(1,dim(X)[2])
lpy.c<-lpy.X(y,X[,z==1,drop=FALSE], g=g, nu0=nu0, s20=s20)



## Set up Gibbs sampler 
S <- 10000

## Storage matrices
Z <- BETA <- SIGMA2 <- NULL

## Gibbs Sampler
for(s in 1:S){
print(s)	
	
## update z	
	for(j in sample(1:p))
    {
		zp<-z
		zp[j]<-1-zp[j]
		
		lpy.p<- lpy.X(y,X[,zp==1,drop=FALSE], g=g, nu0=nu0, s20=s20)
		
		
		
		if(zp[j]==1){
		prob.z.1 <- exp(lpy.p)/(exp(lpy.p) + exp(lpy.c))
		}
		
		if(zp[j]==0){
		prob.z.1 <- exp(lpy.c)/(exp(lpy.p) + exp(lpy.c))
		}
		
		
		z[j]<-rbinom(1, 1, prob.z.1)
		
		lpy.c<- lpy.X(y,X[,z==1,drop=FALSE], g=g, nu0=nu0, s20=s20)
	}
	
## Update sigma.sq 
Hg <- (g/(g+1))*X[,z==1,drop=FALSE]%*%solve(t(X[,z==1,drop=FALSE])%*%X[,z==1,drop=FALSE])%*%t(X[,z==1,drop=FALSE])
SSRg <- t(y)%*%(diag(1, nrow=n) - Hg)%*%y

sigma.sq <- 1/rgamma(1, (nu0 + n)/2 , (nu0*s20 + SSRg)/2)


## Update beta
M.s <- (g/(g+1))*solve(t(X[,z==1,drop=FALSE])%*%X[,z==1,drop=FALSE])%*%t(X[,z==1,drop=FALSE])%*%y
beta.s <- rmvnorm(1, M.s, (g/(g+1))*sigma.sq*solve(t(X[,z==1,drop=FALSE])%*%X[,z==1,drop=FALSE]))
beta <- z
beta[z==1] <- beta.s

## store values
Z <- rbind(Z, z)
BETA <- rbind(BETA, beta)
SIGMA2 <- rbind(SIGMA2, sigma.sq)	
}	


Beta.bma.pm <- apply(BETA, 2, mean)
y.hat.test <- X.test%*%Beta.bma.pm
mean( (y.test-y.hat.test)^2)

Z.prob <- apply(Z, 2, mean)
plot(Z.prob, xlab="regressor index", ylab=expression(paste( "Pr(",italic(z[j] == 1),"|",italic(y),",X)",sep="")), type="h", lwd=2, col="blue")













